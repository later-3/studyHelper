import streamlit as st
from streamlit_option_menu import option_menu
from streamlit_lottie import st_lottie
import requests
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from PIL import Image
import json
import uuid
import re
from dotenv import load_dotenv
import logging
from core import logger_config
from streamlit_elements import elements, mui, html
from typing import List, Dict
from datetime import datetime
import pandas as pd
from components.ui_components import (
    render_stats_overview, render_subject_distribution_chart, 
    render_activity_trend_chart, render_filter_panel,
    render_question_group_card, render_loading_spinner, render_empty_state, COLORS
)

# --- åˆå§‹åŒ–æ—¥å¿— ---
logger_config.setup_logging()
logger = logging.getLogger(__name__)

# --- åªä»æœåŠ¡å±‚å¯¼å…¥ --- 
from services import storage_service, ocr_service, llm_service
from services.data_service import data_service
from core import user_management as um

def load_lottieurl(url: str):
    try:
        r = requests.get(url, timeout=5)
        if r.status_code != 200:
            logger.warning(f"Failed to fetch lottie animation from {url}, status code: {r.status_code}")
            return None
        return r.json()
    except requests.exceptions.RequestException:
        logger.error(f"Could not load Lottie animation from {url}", exc_info=True)
        return None

# --- æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ (å¯è¢«ç‹¬ç«‹æµ‹è¯•) ---
def intelligent_search_logic(user: dict, image_path: str, force_new_analysis: bool = False):
    logger.info(f"Starting intelligent_search_logic for user {user['id']}. Image path: {image_path}, Force new: {force_new_analysis}")
    
    # é¦–å…ˆè¿›è¡ŒOCRè¯†åˆ«
    logger.info("Starting OCR process...")
    ocr_text_list = ocr_service.get_text_from_image(image_path)
    logger.info(f"OCRè¯†åˆ«ç»“æœ: {ocr_text_list}")
    ocr_text = '\n'.join(ocr_text_list)
    logger.info(f"OCRè¯†åˆ«å‡ºçš„æ–‡å­—å¦‚ä¸‹: {ocr_text}")
    
    # æ£€æŸ¥OCRç»“æœæ˜¯å¦æœ‰æ•ˆ
    if not ocr_text.strip() or ocr_text_list[0] in ["è¯†åˆ«å¤±è´¥", "è¯†åˆ«å¼‚å¸¸"]:
        logger.error(f"OCR failed for image: {image_path}, result: {ocr_text}")
        return None, "æ–‡å­—è¯†åˆ«å¤±è´¥æˆ–å›¾ç‰‡ä¸ºç©ºï¼Œè¯·ç¡®ä¿å›¾ç‰‡æ¸…æ™°ã€‚", None, None
    
    logger.info(f"OCR successful, text: {ocr_text[:100]}...")
    
    # å¦‚æœä¸æ˜¯å¼ºåˆ¶é‡æ–°åˆ†æï¼Œå°è¯•ä»ç¼“å­˜è·å–
    if not force_new_analysis:
        # L1ç¼“å­˜ï¼šé€šè¿‡å›¾ç‰‡phashæŸ¥æ‰¾
        existing_question = storage_service.get_question_by_phash(image_path)
        if existing_question and existing_question.get('master_analysis'):
            logger.info(f"L1 Cache HIT. Question ID: {existing_question['question_id']}")
            storage_service.save_submission(user['id'], existing_question['question_id'], "(Image match)")
            # è¿”å›æ–°OCRçš„ç»“æœï¼Œè€Œä¸æ˜¯å†å²æ•°æ®ä¸­çš„canonical_text
            return existing_question['master_analysis'], ocr_text, "phash_hit", existing_question['question_id']
        
        # L2ç¼“å­˜ï¼šé€šè¿‡æ–‡æœ¬hashæŸ¥æ‰¾
        question_id_from_text = storage_service.generate_question_id(ocr_text)
        existing_question = storage_service.get_question_by_id(question_id_from_text)
        if existing_question and existing_question.get('master_analysis'):
            logger.info(f"L2 Cache HIT based on text hash. Question ID: {question_id_from_text}")
            # å°†æ–°å›¾ç‰‡çš„phashå…³è”åˆ°ç°æœ‰é—®é¢˜
            storage_service.add_question(ocr_text, existing_question['master_analysis'], image_path, question_id_from_text)
            storage_service.save_submission(user['id'], question_id_from_text, ocr_text)
            return existing_question['master_analysis'], ocr_text, "text_hash_hit", question_id_from_text

    # ç¼“å­˜æœªå‘½ä¸­æˆ–å¼ºåˆ¶é‡æ–°åˆ†æï¼Œè°ƒç”¨å¤§æ¨¡å‹
    question_id_from_text = storage_service.generate_question_id(ocr_text)
    logger.info(f"Cache MISS or force new analysis. Calling LLM service for Question ID: {question_id_from_text}")
    
    analysis_str = llm_service.get_analysis_for_text(ocr_text)
    match = re.search(r'```json\n({[\s\S]*?})\n```', analysis_str)
    try:
        master_analysis = json.loads(match.group(1) if match else analysis_str)
    except json.JSONDecodeError:
        logger.error(f"Failed to parse LLM JSON response: {analysis_str}", exc_info=True)
        return None, f"AIåˆ†æç»“æœè§£æå¤±è´¥: {analysis_str}", None, None
    
    # ä¿å­˜æ–°çš„åˆ†æç»“æœ
    success = storage_service.add_question(ocr_text, master_analysis, image_path, question_id_from_text)
    if success:
        storage_service.save_submission(user['id'], question_id_from_text, ocr_text)
        logger.info(f"Successfully added new question to bank: {question_id_from_text}")
        cache_status = "miss"
    else:
        logger.error(f"Failed to save question data: {question_id_from_text}")
        return None, "ä¿å­˜åˆ†æç»“æœå¤±è´¥ï¼Œè¯·é‡è¯•ã€‚", None, None
        
    return master_analysis, ocr_text, cache_status, question_id_from_text

# --- UI æ¸²æŸ“å‡½æ•° ---
def render_analysis_results(master_analysis, user, question_id, ocr_text):
    st.subheader("ğŸ’¡ AIåˆ†æç»“æœ")
    if master_analysis.get("is_correct"):
        st.success("### æ­å–œä½ ï¼Œç­”å¯¹äº†ï¼ğŸ‰")
    else:
        st.error("### åˆ«ç°å¿ƒï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹é—®é¢˜å‡ºåœ¨å“ªï¼ŸğŸ¤”")
        if master_analysis.get("error_analysis"): st.markdown(f"**é”™è¯¯åˆ†æ:** {master_analysis['error_analysis']}")
        if master_analysis.get("correct_answer"): st.markdown(f"**æ­£ç¡®ç­”æ¡ˆ:** `{master_analysis['correct_answer']}`")
    
    past_submissions = storage_service.get_submissions_by_question(user['id'], question_id)
    if len(past_submissions) > 1:
        st.warning("âš ï¸ æ‚¨ä»¥å‰ä¹Ÿåšè¿‡è¿™é“é¢˜å“¦ï¼")
        for i, sub in enumerate(past_submissions[1:], 1):
            st.write(f"> ç¬¬{i}æ¬¡æäº¤äº: {sub['timestamp']}, å½“æ—¶æäº¤çš„å†…å®¹æ˜¯: `{sub['submitted_ocr_text']}`")

    st.markdown("---")
    with st.expander("âœ… è§£é¢˜æ­¥éª¤", expanded=True): st.info(master_analysis.get("solution_steps", "æš‚æ— æä¾›"))
    with st.expander("ğŸ§  æ ¸å¿ƒçŸ¥è¯†ç‚¹"): st.success(master_analysis.get("knowledge_point", "æš‚æ— æä¾›"))
    with st.expander("âš ï¸ å¸¸è§æ˜“é”™ç‚¹"): st.warning(master_analysis.get("common_mistakes", "æš‚æ— æä¾›"))

def intelligent_search_page():
    st.title("ğŸ§  æ™ºèƒ½æœé¢˜")
    st.markdown("ä¸Šä¼ é¢˜ç›®ç…§ç‰‡ï¼ŒAIå¯¼å¸ˆå°†ä¸ºæ‚¨æä¾›ç‹¬å®¶åˆ†æã€‚å¦‚æœé¢˜ç›®å·²åœ¨æˆ‘ä»¬çš„çŸ¥è¯†åº“ä¸­ï¼Œæ‚¨å°†ç›´æ¥è·å¾—ç§’çº§å“åº”ï¼")

    if "analysis_results" not in st.session_state:
        st.session_state.analysis_results = None

    uploaded_file = st.file_uploader("æ”¯æŒ jpg, png, jpeg æ ¼å¼çš„å›¾ç‰‡", type=["jpg", "png", "jpeg"])

    if uploaded_file is not None and uploaded_file.file_id != st.session_state.get('current_file_id'):
        st.session_state.current_file_id = uploaded_file.file_id
        st.session_state.analysis_results = None
        logger.info(f"New file uploaded: {uploaded_file.name}, File ID: {uploaded_file.file_id}")
        
        user = st.session_state.current_user
        if not user: st.error("å‘ç”Ÿé”™è¯¯ï¼šæ— æ³•è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯ï¼Œè¯·é‡æ–°ç™»å½•ã€‚"); return

        submission_dir = f"data/submissions/{user['id']}"
        os.makedirs(submission_dir, exist_ok=True)
        image_path = os.path.join(submission_dir, f"{uuid.uuid4().hex[:12]}.png")
        with open(image_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        logger.info(f"Image saved to persistent path: {image_path}")

        with st.spinner("ğŸ¤– AIå¯¼å¸ˆæ­£åœ¨åˆ†æä¸­...ï¼ˆé¦–æ¬¡åˆ†æå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰"):
            master_analysis, ocr_text, cache_status, question_id = intelligent_search_logic(user, image_path)
        
        st.session_state.analysis_results = {
            "master_analysis": master_analysis,
            "ocr_text": ocr_text,
            "cache_status": cache_status,
            "question_id": question_id,
            "image_path": image_path
        }

    if st.session_state.analysis_results:
        res = st.session_state.analysis_results
        master_analysis = res['master_analysis']
        ocr_text = res['ocr_text']
        cache_status = res['cache_status']
        question_id = res['question_id']
        image_path = res['image_path']
        user = st.session_state.current_user

        if not master_analysis:
            st.error(ocr_text)
            return

        col1, col2 = st.columns(2)
        with col1: st.image(image_path, caption="æ‚¨ä¸Šä¼ çš„é¢˜ç›®", use_container_width=True)
        with col2:
            if cache_status == 'phash_hit':
                st.success("âš¡ï¸ å›¾ç‰‡å·²è¯†åˆ«ï¼æˆ‘ä»¬ä»¥å‰è§è¿‡è¿™å¼ å›¾ã€‚")
                st.info("ç›´æ¥ä¸ºæ‚¨å±•ç¤ºå†å²åˆ†æç»“æœã€‚")
            elif cache_status == 'text_hash_hit':
                st.success("âš¡ï¸ é¢˜ç›®å†…å®¹å·²è¯†åˆ«ï¼")
                st.info("è™½ç„¶å›¾ç‰‡æ˜¯æ–°çš„ï¼Œä½†æˆ‘ä»¬åšè¿‡è¿™é“é¢˜ã€‚")
            else:
                st.info("âœ¨ å…¨æ–°é¢˜ç›®ï¼å·²ä¸ºæ‚¨æ°¸ä¹…å­˜å…¥çŸ¥è¯†åº“ï¼")
            
            st.text_area("è¯†åˆ«å‡ºçš„æ–‡å­—å¦‚ä¸‹ï¼š", ocr_text, height=100)
            logger.info(f"è¯†åˆ«å‡ºçš„æ–‡å­—å¦‚ä¸‹ï¼š{ocr_text}")

        if st.button("å¼ºåˆ¶ä½¿ç”¨å¤§æ¨¡å‹é‡æ–°åˆ†æ", help="å¦‚æœæ‚¨è®¤ä¸ºAIçš„åˆ†æç»“æœä¸å¤Ÿç†æƒ³ï¼Œå¯ä»¥å¼ºåˆ¶ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œä¸€æ¬¡å…¨æ–°çš„åˆ†æ"):
            with st.spinner("ğŸ¤– æ­£åœ¨å¼ºåˆ¶è°ƒç”¨å¤§æ¨¡å‹...è¯·ç¨å€™"):
                master_analysis, ocr_text, cache_status, question_id = intelligent_search_logic(user, image_path, force_new_analysis=True)
            st.session_state.analysis_results['master_analysis'] = master_analysis
            st.success("âœ… å·²è·å–æœ€æ–°çš„AIåˆ†æç»“æœï¼")
            st.rerun()

        render_analysis_results(master_analysis, user, question_id, ocr_text)

# ... (home, about, user_management_page, etc. remain the same)

def home():
    st.title("æ¬¢è¿ä½¿ç”¨ StudyHelper æ™ºèƒ½å­¦ä¹ åŠ©æ‰‹ ğŸ‘‹")
    st.subheader("æ‚¨èº«è¾¹çš„AIå­¦ä¹ ä¼™ä¼´")
    lottie_url = "https://lottie.host/embed/a7b5c79a-18c7-4bb9-9a24-9aacb741b330/2Jp4k5t9kM.json"
    lottie_json = load_lottieurl(lottie_url)
    if lottie_json:
        st_lottie(lottie_json, speed=1, width=600, height=300, key="study_lottie")
    st.markdown(
        """
        **æœ¬åº”ç”¨è‡´åŠ›äºé€šè¿‡AIæŠ€æœ¯ï¼Œå¸®åŠ©æ‚¨æ›´é«˜æ•ˆåœ°å­¦ä¹ ï¼š**
        - **`æ™ºèƒ½æœé¢˜`**: ä¸Šä¼ é¢˜ç›®å›¾ç‰‡ï¼ŒAIå°†ä¸ºæ‚¨æä¾›è¯¦ç»†çš„è§£é¢˜æ€è·¯ã€çŸ¥è¯†ç‚¹å’Œæ˜“é”™ç‚¹åˆ†æã€‚
        - **`ç­”é¢˜å†å²`**: è‡ªåŠ¨æ”¶å½•æ‚¨åˆ†æè¿‡çš„æ‰€æœ‰é¢˜ç›®ï¼Œæ–¹ä¾¿éšæ—¶å›é¡¾å¤ä¹ ã€‚
        - **`ä¸¾ä¸€åä¸‰`**: (å³å°†æ¨å‡º) æ ¹æ®æ‚¨çš„é—®é¢˜ï¼Œä¸ºæ‚¨æ¨èåŒç±»å‹çš„ç»ƒä¹ é¢˜ã€‚

        ğŸ‘ˆ è¯·ä»å·¦ä¾§å¯¼èˆªæ é€‰æ‹©åŠŸèƒ½å¼€å§‹ä½¿ç”¨ï¼
        """
    )

def submission_history_page():
    """ä¼˜åŒ–çš„ç­”é¢˜å†å²é¡µé¢"""
    st.title("ğŸ“– ç­”é¢˜å†å²")
    
    user = st.session_state.current_user
    if not user:
        st.error("è¯·å…ˆç™»å½•")
        return
    
    # åˆå§‹åŒ–session state
    if 'history_view_mode' not in st.session_state:
        st.session_state.history_view_mode = 'grouped'  # grouped, timeline, stats
    
    # è·å–ç”¨æˆ·æäº¤è®°å½•
    with st.spinner("åŠ è½½æ•°æ®ä¸­..."):
        submissions = data_service.get_user_submissions(user['id'], user['role'])
    
    if not submissions:
        render_empty_state("æš‚æ— ç­”é¢˜è®°å½•", "ğŸ“")
        return
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = data_service.get_submission_stats(submissions)
    
    # é¡µé¢å¸ƒå±€
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        st.subheader("ğŸ“Š å­¦ä¹ æ¦‚è§ˆ")
        render_stats_overview(stats)
    
    with col2:
        st.subheader("ğŸ“ˆ å­¦ç§‘åˆ†å¸ƒ")
        render_subject_distribution_chart(stats)
    
    with col3:
        st.subheader("ğŸ“… æ´»åŠ¨è¶‹åŠ¿")
        render_activity_trend_chart(stats)
    
    st.markdown("---")
    
    # è§†å›¾æ¨¡å¼åˆ‡æ¢
    view_mode = st.radio(
        "æŸ¥çœ‹æ¨¡å¼",
        options=[
            ("grouped", "ğŸ“š æŒ‰é¢˜ç›®åˆ†ç»„"),
            ("timeline", "â° æ—¶é—´çº¿"),
            ("stats", "ğŸ“Š è¯¦ç»†ç»Ÿè®¡")
        ],
        format_func=lambda x: x[1],
        key="view_mode_radio"
    )
    st.session_state.history_view_mode = view_mode[0]
    
    # ç­›é€‰é¢æ¿
    subjects = list(stats['subject_distribution'].keys())
    filter_params = render_filter_panel(subjects)
    
    # åº”ç”¨ç­›é€‰
    correctness_map = {"æ­£ç¡®": True, "é”™è¯¯": False, "æœªçŸ¥": None}
    correctness_values = [correctness_map.get(c) for c in filter_params['correctness']]
    filtered_submissions = data_service.search_submissions(
        user['id'], 
        user['role'],
        subjects=filter_params['subjects'],
        correctness=correctness_values,
        date_range=filter_params['date_range']
    )
    
    if not filtered_submissions:
        render_empty_state("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆç­›é€‰æ¡ä»¶çš„è®°å½•", "ğŸ”")
        return
    
    # æ ¹æ®è§†å›¾æ¨¡å¼æ¸²æŸ“å†…å®¹
    if st.session_state.history_view_mode == 'grouped':
        render_grouped_view(filtered_submissions)
    elif st.session_state.history_view_mode == 'timeline':
        render_timeline_view(filtered_submissions)
    elif st.session_state.history_view_mode == 'stats':
        render_stats_view(filtered_submissions)

def render_grouped_view(submissions: List[Dict]):
    """æ¸²æŸ“åˆ†ç»„è§†å›¾"""
    st.subheader("ğŸ“š æŒ‰é¢˜ç›®åˆ†ç»„")
    
    # æŒ‰é¢˜ç›®åˆ†ç»„
    grouped_submissions = data_service.group_submissions_by_question(submissions)
    
    if not grouped_submissions:
        render_empty_state("æš‚æ— åˆ†ç»„æ•°æ®", "ğŸ“š")
        return
    
    # æ˜¾ç¤ºåˆ†ç»„ç»Ÿè®¡
    st.info(f"å…±æ‰¾åˆ° {len(grouped_submissions)} é“ä¸åŒçš„é¢˜ç›®")
    
    # æ¸²æŸ“æ¯ä¸ªé¢˜ç›®ç»„
    for question_id, question_submissions in grouped_submissions.items():
        # è·å–é¢˜ç›®è¯¦æƒ…
        question_details = data_service.get_question_details(question_id)
        
        # æ¸²æŸ“é¢˜ç›®ç»„å¡ç‰‡
        render_question_group_card(question_id, question_submissions, question_details)

def render_timeline_view(submissions: List[Dict]):
    """æ¸²æŸ“æ—¶é—´çº¿è§†å›¾"""
    st.subheader("â° æ—¶é—´çº¿")
    
    # æŒ‰æ—¶é—´æ’åº
    sorted_submissions = sorted(submissions, key=lambda x: x.get('timestamp', ''), reverse=True)
    
    # æŒ‰æ—¥æœŸåˆ†ç»„
    date_groups = {}
    for submission in sorted_submissions:
        timestamp = submission.get('timestamp', '')
        if timestamp:
            try:
                date = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                date_str = date.strftime('%Yå¹´%mæœˆ%dæ—¥')
                if date_str not in date_groups:
                    date_groups[date_str] = []
                date_groups[date_str].append(submission)
            except:
                continue
    
        # æ¸²æŸ“æ—¶é—´çº¿
    for date_str, day_submissions in date_groups.items():
        st.write(f"### {date_str}")
        
        for submission in day_submissions:
            # è·å–åˆ†æç»“æœ
            ai_analysis = submission.get('ai_analysis')
            if ai_analysis:
                subject = ai_analysis.get('subject', 'æœªæŒ‡å®š')
                is_correct = ai_analysis.get('is_correct')
                q_text = submission.get('ocr_text', submission.get('submitted_ocr_text', ''))
            else:
                question_id = submission.get('question_id')
                question = data_service.get_question_details(question_id)
                if question:
                    subject = question.get('subject', 'æœªæŒ‡å®š')
                    is_correct = question.get('master_analysis', {}).get('is_correct')
                    q_text = question.get('canonical_text', '')
                else:
                    subject = 'æœªæŒ‡å®š'
                    is_correct = None
                    q_text = submission.get('submitted_ocr_text', '')
            
            # æ ¼å¼åŒ–æ—¶é—´
            timestamp = submission.get('timestamp', '')
            if timestamp:
                try:
                    date = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                    time_str = date.strftime('%H:%M')
                except:
                    time_str = timestamp
            else:
                time_str = 'æœªçŸ¥æ—¶é—´'
            
            # çŠ¶æ€å›¾æ ‡
            status_icon = "âœ…" if is_correct is True else "âŒ" if is_correct is False else "â“"
            
            # æ˜¾ç¤ºä¿¡æ¯
            col1, col2, col3, col4 = st.columns([1, 1, 2, 6])
            with col1:
                st.write(time_str)
            with col2:
                st.write(status_icon)
            with col3:
                st.write(f"**{subject}**")
            with col4:
                st.write(q_text[:50] + "..." if len(q_text) > 50 else q_text)
            
            st.divider()

def render_stats_view(submissions: List[Dict]):
    """æ¸²æŸ“ç»Ÿè®¡è§†å›¾"""
    st.subheader("ğŸ“Š è¯¦ç»†ç»Ÿè®¡")
    
    # è·å–è¯¦ç»†ç»Ÿè®¡
    stats = data_service.get_submission_stats(submissions)
    
    # å­¦ç§‘è¯¦ç»†ç»Ÿè®¡
    st.write("### å­¦ç§‘è¡¨ç°")
    if stats['subject_distribution']:
        subject_data = []
        for subject, count in stats['subject_distribution'].items():
            # è®¡ç®—è¯¥å­¦ç§‘çš„æ­£ç¡®ç‡
            subject_submissions = [s for s in submissions if 
                                 (s.get('ai_analysis', {}).get('subject') == subject) or
                                 (data_service.get_question_details(s.get('question_id', {})) or {}).get('subject') == subject]
            
            correct_count = sum(1 for s in subject_submissions if 
                              (s.get('ai_analysis', {}).get('is_correct') is True) or
                              (data_service.get_question_details(s.get('question_id', {})) or {}).get('master_analysis', {}).get('is_correct') is True)
            
            accuracy = (correct_count / count * 100) if count > 0 else 0
            subject_data.append({
                'å­¦ç§‘': subject,
                'é¢˜æ•°': count,
                'æ­£ç¡®æ•°': correct_count,
                'æ­£ç¡®ç‡': f"{accuracy:.1f}%"
            })
        
        # æ˜¾ç¤ºè¡¨æ ¼
        df = pd.DataFrame(subject_data)
        st.dataframe(df, use_container_width=True)
    
    # æœ€è¿‘æ´»åŠ¨è¯¦ç»†ç»Ÿè®¡
    st.write("### æœ€è¿‘æ´»åŠ¨")
    if stats['recent_activity']:
        activity_data = []
        for date, count in sorted(stats['recent_activity'].items()):
            activity_data.append({
                'æ—¥æœŸ': date,
                'æäº¤æ¬¡æ•°': count
            })
        
        df = pd.DataFrame(activity_data)
        st.dataframe(df, use_container_width=True)

def about():
    st.title("â„¹ï¸ å…³äº StudyHelper")
    st.info(
        """
        **ç‰ˆæœ¬:** 7.1.0 (ç±»å‹ä¿®å¤)
        **é¡¹ç›®æ„¿æ™¯:** æ‰“é€ ä¸€æ¬¾æ‡‚ä½ çš„æ™ºèƒ½å­¦ä¹ åŠ©æ‰‹ã€‚
        """
    )

def user_management_page():
    st.title("ğŸ‘¥ ç”¨æˆ·ç®¡ç†")
    user = st.session_state.current_user
    if not user: st.warning("è¯·å…ˆç™»å½•ä»¥æŸ¥çœ‹æ­¤é¡µé¢ã€‚"); return

    st.markdown(f"#### æ¬¢è¿ï¼Œ{user['name']}!")

    with elements(f"user_dashboard_{user['id']}"):
        if user['role'] == 'school':
            st.subheader("ğŸ« å­¦æ ¡ä»ªè¡¨ç›˜")
            all_teachers, all_students, all_classes = um.get_all_teachers(), um.get_all_students(), um.get_all_classes()
            with mui.Grid(container=True, spacing=2):
                with mui.Grid(item=True, xs=6):
                    with mui.Paper(elevation=3, sx={"padding": 2, "textAlign": 'center'}):
                        mui.Typography("ğŸ‘¨â€ğŸ« æ•™å¸ˆæ€»æ•°", variant="h6")
                        mui.Typography(len(all_teachers), variant="h4")
                with mui.Grid(item=True, xs=6):
                    with mui.Paper(elevation=3, sx={"padding": 2, "textAlign": 'center'}):
                        mui.Typography("ğŸ“ å­¦ç”Ÿæ€»æ•°", variant="h6")
                        mui.Typography(len(all_students), variant="h4")
            
            for cls in all_classes:
                teacher_name = next((t['name'] for t in all_teachers if cls['id'] in t.get('manages_classes', [])), 'æš‚æ— ')
                with mui.Card(key=cls['id'], sx={"marginTop": 2}):
                    mui.CardHeader(title=cls['name'], subheader=f"ç­ä¸»ä»»: {teacher_name}")
                    with mui.CardContent:
                        for student in um.get_students_by_class(cls['id']):
                            mui.Chip(label=student['name'], variant="outlined", sx={"marginRight": 0.5, "marginBottom": 0.5})

        elif user['role'] == 'teacher':
            st.subheader("ğŸ‘¨â€ğŸ« æˆ‘ç®¡ç†çš„ç­çº§")
            managed_classes = um.get_classes_by_teacher(user['id'])
            if not managed_classes: st.info("æ‚¨ç›®å‰æ²¡æœ‰ç®¡ç†ä»»ä½•ç­çº§ã€‚"); return
            for cls in managed_classes:
                with mui.Card(key=cls['id'], sx={"marginTop": 2}):
                    mui.CardHeader(title=cls['name'])
                    with mui.CardContent:
                        for student in um.get_students_by_class(cls['id']):
                            mui.Chip(label=student['name'], variant="outlined", sx={"marginRight": 0.5, "marginBottom": 0.5})

        elif user['role'] == 'student':
            st.subheader("ğŸ§‘â€ğŸ“ æˆ‘çš„ä¿¡æ¯")
            class_id = user.get('class_id')
            all_classes = um.get_all_classes()
            class_name = next((c['name'] for c in all_classes if c['id'] == class_id), "æœªåˆ†é…ç­çº§")
            teacher_name = "æš‚æ— "
            if class_id:
                all_teachers = um.get_all_teachers()
                teacher_name = next((t['name'] for t in all_teachers if class_id in t.get('manages_classes', [])), "æš‚æ— ")
            with mui.Card(sx={"padding": 2}):
                mui.Typography(f"å§“å: {user['name']}", variant="h6")
                mui.Typography(f"ç­çº§: {class_name}", variant="body1")
                mui.Typography(f"ç­ä¸»ä»»: {teacher_name}", variant="body1")

# --- ä¸»ç¨‹åº ---
load_dotenv("/Users/xulater/studyHelper/studyhelper-demo/studyhelper-demo-final/.env")
st.set_page_config(page_title="StudyHelper", page_icon="ğŸ“˜", layout="wide")

if 'current_user' not in st.session_state: st.session_state.current_user = None

with st.sidebar:
    if st.session_state.current_user:
        user = st.session_state.current_user
        role_icon = {"school": "ğŸ«", "teacher": "ğŸ‘¨â€ğŸ«", "student": "ğŸ§‘â€ğŸ“"}
        with elements("sidebar_user_card"):
            with mui.Paper(elevation=3, sx={"padding": 1, "marginBottom": 1}):
                mui.Typography(f"{role_icon.get(user['role'], 'ğŸ‘¤')} {user['name']}", variant="h6")
                mui.Typography(f"è§’è‰²: {user['role']}", variant="body2")
        if st.button("é€€å‡ºç™»å½•", use_container_width=True, type="primary"): 
            st.session_state.current_file_id = None
            st.session_state.analysis_results = None
            st.session_state.current_user = None
            st.rerun()
    else:
        st.info("è¯·é€‰æ‹©ç”¨æˆ·ç™»å½•")
        users = um.get_all_users()
        user_options = {user['id']: f"{user['name']} ({user['role']})" for user in users}
        selected_user_id = st.selectbox("é€‰æ‹©ç”¨æˆ·", options=list(user_options.keys()), format_func=lambda x: user_options[x], index=None, placeholder="è¯·é€‰æ‹©...")
        if st.button("ç™»å½•", use_container_width=True):
            if selected_user_id: 
                st.session_state.current_user = um.get_user_by_id(selected_user_id)
                st.session_state.analysis_results = None
                st.rerun()
            else: st.warning("è¯·å…ˆé€‰æ‹©ä¸€ä¸ªç”¨æˆ·")
    st.markdown("---")

    options = ["é¦–é¡µ", "å…³äº"]
    icons = ["house-door-fill", "info-circle-fill"]
    if st.session_state.current_user:
        options.insert(1, "ç”¨æˆ·ç®¡ç†")
        icons.insert(1, "people-fill")
        if st.session_state.current_user['role'] in ['student', 'teacher', 'school']:
            options.insert(2, "ç­”é¢˜å†å²")
            icons.insert(2, "card-checklist")
        if st.session_state.current_user['role'] in ['student', 'teacher']:
            options.insert(2, "æ™ºèƒ½æœé¢˜")
            icons.insert(2, "search-heart-fill")

    selected = option_menu(
        menu_title="StudyHelper", options=options, icons=icons, menu_icon="book-half", default_index=0,
        styles={
            "container": {"padding": "5!important", "background-color": "#fafafa"},
            "icon": {"color": "orange", "font-size": "25px"}, 
            "nav-link": {"color": "#333333", "font-size": "16px", "text-align": "left", "margin":"0px", "--hover-color": "#eee"},
            "nav-link-selected": {"background-color": "#02ab21"},
        }
    )

# é¡µé¢è·¯ç”±
if selected == "é¦–é¡µ": home()
elif selected == "ç”¨æˆ·ç®¡ç†": user_management_page()
elif selected == "æ™ºèƒ½æœé¢˜":
    if st.session_state.current_user: intelligent_search_page()
    else: st.warning("è¯·ä»ä¾§è¾¹æ ç™»å½•ä»¥ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚")
elif selected == "ç­”é¢˜å†å²":
    if st.session_state.current_user: submission_history_page()
    else: st.warning("è¯·ä»ä¾§è¾¹æ ç™»å½•ä»¥ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚")
elif selected == "å…³äº": about()